{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Loading"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200812181037-0000\n",
      "KERNEL_ID = ab6d100c-41a2-44b5-9950-b81698638ae3\n"
     ]
    }
   ],
   "source": [
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': '****',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': '****'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_8cbf59ff782d4a5db7683b1dceef1190_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "\n",
    "from pyspark import SparkContext, SparkConf\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "\n",
    "# Load the deplyment sample dataset\n",
    "df = spark.read.option(\"header\",True) \\\n",
    "     .option('inferSchema','true')\\\n",
    "     .csv(cos.url('Deployment_Sample_Data.csv', 'advanceddatasciencecapstone-donotdelete-pr-****'))\n",
    "\n",
    "\n",
    "# Load the LSM_Imputer table to impute the missing values\n",
    "df_lsm=spark.read.parquet(cos.url('LSM_Imputer.parquet', 'advanceddatasciencecapstone-donotdelete-pr-****'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load saved model files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import types\n",
    "import pandas as pd\n",
    "from botocore.client import Config\n",
    "import ibm_boto3\n",
    "\n",
    "def __iter__(self): return 0\n",
    "\n",
    "# @hidden_cell\n",
    "# The following code accesses a file in your IBM Cloud Object Storage. It includes your credentials.\n",
    "# You might want to remove those credentials before you share the notebook.\n",
    "client_8cbf59ff782d4a5db7683b1dceef1190 = ibm_boto3.client(service_name='s3',\n",
    "    ibm_api_key_id='****',\n",
    "    ibm_auth_endpoint=\"https://iam.cloud.ibm.com/oidc/token\",\n",
    "    config=Config(signature_version='oauth'),\n",
    "    endpoint_url='https://s3-api.us-geo.objectstorage.service.networklayer.com')\n",
    "\n",
    "# Your data file was loaded into a botocore.response.StreamingBody object.\n",
    "# Please read the documentation of ibm_boto3 and pandas to learn more about the possibilities to load the data.\n",
    "# ibm_boto3 documentation: https://ibm.github.io/ibm-cos-sdk-python/\n",
    "# pandas documentation: http://pandas.pydata.org/\n",
    "streaming_lr_model = client_8cbf59ff782d4a5db7683b1dceef1190.get_object(Bucket='advanceddatasciencecapstone-donotdelete-pr-****', Key='LR_Model_Final.mdl.tar')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(streaming_lr_model, \"__iter__\"): streaming_lr_model.__iter__ = types.MethodType( __iter__, streaming_lr_model ) \n",
    "\n",
    "    \n",
    "streaming_pipeline_model = client_8cbf59ff782d4a5db7683b1dceef1190.get_object(Bucket='advanceddatasciencecapstone-donotdelete-pr-****', Key='Transformation_Pipeline.ppl.tar')['Body']\n",
    "# add missing __iter__ method, so pandas accepts body as file-like object\n",
    "if not hasattr(streaming_pipeline_model, \"__iter__\"): streaming_pipeline_model.__iter__ = types.MethodType( __iter__, streaming_pipeline_model ) \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model files\n",
    "def save_stream(input_stream, output_filename):\n",
    "    \n",
    "    buff=input_stream.read()\n",
    "    f = open(output_filename,\"wb\")\n",
    "    f.write(buff)\n",
    "    f.close()\n",
    "    \n",
    "# save Logistic Regression Model file\n",
    "save_stream(streaming_lr_model, \"LR_Model_Final.mdl.tar\")\n",
    "\n",
    "# save Pipeline Model file\n",
    "save_stream(streaming_pipeline_model, \"Transformation_Pipeline.ppl.tar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LR_Model_Final.mdl/\n",
      "LR_Model_Final.mdl/data/\n",
      "LR_Model_Final.mdl/data/.part-00000-8afd7252-7efb-4e4d-87ac-9983903394cc-c000.snappy.parquet.crc\n",
      "LR_Model_Final.mdl/data/._SUCCESS.crc\n",
      "LR_Model_Final.mdl/data/part-00000-8afd7252-7efb-4e4d-87ac-9983903394cc-c000.snappy.parquet\n",
      "LR_Model_Final.mdl/data/_SUCCESS\n",
      "LR_Model_Final.mdl/metadata/\n",
      "LR_Model_Final.mdl/metadata/.part-00000.crc\n",
      "LR_Model_Final.mdl/metadata/._SUCCESS.crc\n",
      "LR_Model_Final.mdl/metadata/part-00000\n",
      "LR_Model_Final.mdl/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/\n",
      "Transformation_Pipeline.ppl/metadata/\n",
      "Transformation_Pipeline.ppl/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/data/\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/data/.part-00000-81a0f6ba-9de7-44c4-9b62-0023712583c6-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/data/part-00000-81a0f6ba-9de7-44c4-9b62-0023712583c6-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/metadata/\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/00_StringIndexer_4aacbf961efb/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/data/\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/data/.part-00000-4717cede-ead0-44ef-a14a-bb30f40f6ec7-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/data/part-00000-4717cede-ead0-44ef-a14a-bb30f40f6ec7-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/metadata/\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/01_StringIndexer_eecf54fff811/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/data/\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/data/.part-00000-c48b41c2-6495-48b3-baeb-aa3337d081aa-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/data/part-00000-c48b41c2-6495-48b3-baeb-aa3337d081aa-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/metadata/\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/02_StringIndexer_f026d4fb94fe/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/data/\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/data/.part-00000-39717f65-dc1d-4dae-8ea4-419755de7415-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/data/part-00000-39717f65-dc1d-4dae-8ea4-419755de7415-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/metadata/\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/03_StringIndexer_225d279062c7/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/data/\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/data/.part-00000-fc80e07b-df21-4b1b-ba55-c68850dd1e1e-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/data/part-00000-fc80e07b-df21-4b1b-ba55-c68850dd1e1e-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/metadata/\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/04_StringIndexer_4bdf1bea502c/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/data/\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/data/.part-00000-bae00599-42a8-4e28-acc6-7e12b5b36291-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/data/part-00000-bae00599-42a8-4e28-acc6-7e12b5b36291-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/metadata/\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/05_StringIndexer_cc2de24af390/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/data/\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/data/.part-00000-90418148-0c7f-4ff9-b554-d8a4c37b91b7-c000.snappy.parquet.crc\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/data/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/data/part-00000-90418148-0c7f-4ff9-b554-d8a4c37b91b7-c000.snappy.parquet\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/data/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/metadata/\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/06_StringIndexer_bf65c6fb022b/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/07_OneHotEncoder_baa02d9a5cd9/\n",
      "Transformation_Pipeline.ppl/stages/07_OneHotEncoder_baa02d9a5cd9/metadata/\n",
      "Transformation_Pipeline.ppl/stages/07_OneHotEncoder_baa02d9a5cd9/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/07_OneHotEncoder_baa02d9a5cd9/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/07_OneHotEncoder_baa02d9a5cd9/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/07_OneHotEncoder_baa02d9a5cd9/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/08_OneHotEncoder_ee4996f1592d/\n",
      "Transformation_Pipeline.ppl/stages/08_OneHotEncoder_ee4996f1592d/metadata/\n",
      "Transformation_Pipeline.ppl/stages/08_OneHotEncoder_ee4996f1592d/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/08_OneHotEncoder_ee4996f1592d/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/08_OneHotEncoder_ee4996f1592d/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/08_OneHotEncoder_ee4996f1592d/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/09_OneHotEncoder_1eeb378c4bed/\n",
      "Transformation_Pipeline.ppl/stages/09_OneHotEncoder_1eeb378c4bed/metadata/\n",
      "Transformation_Pipeline.ppl/stages/09_OneHotEncoder_1eeb378c4bed/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/09_OneHotEncoder_1eeb378c4bed/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/09_OneHotEncoder_1eeb378c4bed/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/09_OneHotEncoder_1eeb378c4bed/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/10_OneHotEncoder_ad90746bb780/\n",
      "Transformation_Pipeline.ppl/stages/10_OneHotEncoder_ad90746bb780/metadata/\n",
      "Transformation_Pipeline.ppl/stages/10_OneHotEncoder_ad90746bb780/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/10_OneHotEncoder_ad90746bb780/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/10_OneHotEncoder_ad90746bb780/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/10_OneHotEncoder_ad90746bb780/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/11_OneHotEncoder_754098c5b8f2/\n",
      "Transformation_Pipeline.ppl/stages/11_OneHotEncoder_754098c5b8f2/metadata/\n",
      "Transformation_Pipeline.ppl/stages/11_OneHotEncoder_754098c5b8f2/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/11_OneHotEncoder_754098c5b8f2/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/11_OneHotEncoder_754098c5b8f2/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/11_OneHotEncoder_754098c5b8f2/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/12_OneHotEncoder_0353325c52dc/\n",
      "Transformation_Pipeline.ppl/stages/12_OneHotEncoder_0353325c52dc/metadata/\n",
      "Transformation_Pipeline.ppl/stages/12_OneHotEncoder_0353325c52dc/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/12_OneHotEncoder_0353325c52dc/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/12_OneHotEncoder_0353325c52dc/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/12_OneHotEncoder_0353325c52dc/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/13_OneHotEncoder_f81c84c57e29/\n",
      "Transformation_Pipeline.ppl/stages/13_OneHotEncoder_f81c84c57e29/metadata/\n",
      "Transformation_Pipeline.ppl/stages/13_OneHotEncoder_f81c84c57e29/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/13_OneHotEncoder_f81c84c57e29/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/13_OneHotEncoder_f81c84c57e29/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/13_OneHotEncoder_f81c84c57e29/metadata/_SUCCESS\n",
      "Transformation_Pipeline.ppl/stages/14_VectorAssembler_94ad4575ea2b/\n",
      "Transformation_Pipeline.ppl/stages/14_VectorAssembler_94ad4575ea2b/metadata/\n",
      "Transformation_Pipeline.ppl/stages/14_VectorAssembler_94ad4575ea2b/metadata/.part-00000.crc\n",
      "Transformation_Pipeline.ppl/stages/14_VectorAssembler_94ad4575ea2b/metadata/._SUCCESS.crc\n",
      "Transformation_Pipeline.ppl/stages/14_VectorAssembler_94ad4575ea2b/metadata/part-00000\n",
      "Transformation_Pipeline.ppl/stages/14_VectorAssembler_94ad4575ea2b/metadata/_SUCCESS\n"
     ]
    }
   ],
   "source": [
    "# extract compressed models\n",
    "\n",
    "!tar xvf LR_Model_Final.mdl.tar\n",
    "\n",
    "!tar xvf Transformation_Pipeline.ppl.tar"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Missing values treatment / Imputing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Treating the missing values\n",
    "\n",
    "# province\n",
    "df = df.na.fill(\"Unknown\", 'province')\n",
    "\n",
    "# years_occupation\n",
    "df = df.na.fill(0, 'years_occupation')\n",
    "\n",
    "# marital status\n",
    "df = df.na.fill(\"Unknown\", 'marital_status')\n",
    "\n",
    "\n",
    "# LSM\n",
    "\n",
    "# Create a new temporary view of DataFrames\n",
    "df.createOrReplaceTempView(\"dftable\")\n",
    "\n",
    "df_lsm.createOrReplaceTempView(\"df_lsm\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT d.*, CASE WHEN d.lsm IS NULL THEN l.med_lsm ELSE d.lsm END AS lsm_fixed \n",
    "FROM dftable d\n",
    "LEFT JOIN df_lsm l ON l.province=d.province AND l.income_code=d.income_code\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# drop old lsm column\n",
    "df=df.drop('lsm')\n",
    "\n",
    "# rename lsm_fixed back to lsm\n",
    "df=df.withColumnRenamed('lsm_fixed','lsm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove unnecessary values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new temporary view of DataFrame\n",
    "df.createOrReplaceTempView(\"dftable\")\n",
    "\n",
    "# keep DIRECTORSHIP ACTIVE and DIRECTORSHIP RESIGNED only and change others to NO DIRECTORSHIP\n",
    "df=spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "CASE WHEN (directorship<>'DIRECTORSHIP ACTIVE' AND directorship<>'DIRECTORSHIP RESIGNED') \n",
    "     THEN 'NO DIRECTORSHIP'\n",
    "     ELSE directorship \n",
    "END AS directorship_fixed\n",
    "FROM dftable\n",
    "\"\"\")\n",
    "\n",
    "# drop the old directorship column\n",
    "df=df.drop('directorship')\n",
    "\n",
    "# rename directorship_fixed column to directorship\n",
    "df=df.withColumnRenamed('directorship_fixed','directorship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove correlated variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('months_risk_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove outliers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset and update outliers\n",
    "\n",
    "df.createOrReplaceTempView('dftable')\n",
    "\n",
    "df=spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "CASE WHEN months_mobile_phone>300 THEN 300 ELSE months_mobile_phone END AS months_mobile_phone_fixed,\n",
    "CASE WHEN months_home_phone>300 THEN 300 ELSE months_home_phone END AS months_home_phone_fixed,\n",
    "CASE WHEN months_work_phone>300 THEN 300 ELSE months_work_phone END AS months_work_phone_fixed,\n",
    "CASE WHEN years_occupation>35 THEN 35 ELSE years_occupation END AS years_occupation_fixed\n",
    "FROM dftable\n",
    "\"\"\")\n",
    "\n",
    "#drop old columns\n",
    "dropped_cols = ['months_mobile_phone','months_home_phone','months_work_phone','years_occupation']\n",
    "df=df.drop(*dropped_cols)\n",
    "\n",
    "# rename the new fixed columns back to the original names\n",
    "for col in dropped_cols:\n",
    "    df=df.withColumnRenamed(col+'_fixed',col)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create age-group\n",
    "\n",
    "df=df.drop('age_group')\n",
    "\n",
    "df.createOrReplaceTempView('dftable')\n",
    "df=spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "CASE WHEN age_buy_property >= 20 AND age_buy_property <  25 THEN '20-24'\n",
    "    WHEN age_buy_property >= 25 AND age_buy_property <  30 THEN '25-29'\n",
    "    WHEN age_buy_property >= 30 AND age_buy_property <  35 THEN '30-34'\n",
    "    WHEN age_buy_property >= 35 AND age_buy_property <  40 THEN '35-39'\n",
    "    WHEN age_buy_property >= 40 AND age_buy_property <  45 THEN '40-44'\n",
    "    WHEN age_buy_property >= 45 AND age_buy_property <  50 THEN '45-49'\n",
    "    WHEN age_buy_property >= 50 AND age_buy_property <  55 THEN '50-54'\n",
    "    WHEN age_buy_property >= 55 AND age_buy_property <  60 THEN '55-59'\n",
    "    WHEN age_buy_property >= 60 AND age_buy_property <  65 THEN '60-64'\n",
    "    WHEN age_buy_property >= 65 AND age_buy_property <= 80 THEN '65-80'\n",
    "    ELSE 'Unknown'\n",
    "END AS age_group\n",
    "FROM dftable    \n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Drop less important features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dropped less important features\n",
    "df=df.drop('months_home_phone','months_mobile_phone','and months_work_phone','contactability_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Transformation pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import PipelineModel\n",
    "\n",
    "pipelineModel=PipelineModel.load(\"Transformation_Pipeline.ppl\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.classification import LogisticRegressionModel\n",
    "\n",
    "lr = LogisticRegressionModel.load(\"LR_Model_Final.mdl\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transform and Predict data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transform the data using the loaded pipeline\n",
    "df_transformed=pipelineModel.transform(df)\n",
    "\n",
    "# predict the data using the loaded model\n",
    "predictions=lr.transform(df_transformed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save the predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import monotonically_increasing_id, row_number\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "#add 'sequential' index and join both dataframe to get the final result\n",
    "a = df.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "b = predictions.withColumn(\"row_idx\", row_number().over(Window.orderBy(monotonically_increasing_id())))\n",
    "\n",
    "# keep row_idx and prediction columns\n",
    "b=b['row_idx','prediction']\n",
    "\n",
    "# add prediction column to the dataframe\n",
    "final_df = a.join(b, a.row_idx == b.row_idx).\\\n",
    "             drop(\"row_idx\")\n",
    "\n",
    "# save the prediction result as a CSV file\n",
    "final_df.write.csv(header=True, mode='overwrite', path=cos.url('Deployment_output.csv', 'advanceddatasciencecapstone-donotdelete-pr-****'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
