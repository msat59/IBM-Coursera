{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for a Spark session to start...\n",
      "Spark Initialization Done! ApplicationId = app-20200812135848-0000\n",
      "KERNEL_ID = 14e04441-ede2-4219-ba63-cce0e69be2be\n"
     ]
    }
   ],
   "source": [
    "# Load the CSV file and identify the data types automatically using .option('inferSchema','true')\n",
    "\n",
    "import ibmos2spark\n",
    "# @hidden_cell\n",
    "credentials = {\n",
    "    'endpoint': 'https://s3-api.us-geo.objectstorage.service.networklayer.com',\n",
    "    'service_id': '****',\n",
    "    'iam_service_endpoint': 'https://iam.cloud.ibm.com/oidc/token',\n",
    "    'api_key': '***'\n",
    "}\n",
    "\n",
    "configuration_name = 'os_8cbf59ff782d4a5db7683b1dceef1190_configs'\n",
    "cos = ibmos2spark.CloudObjectStorage(sc, credentials, configuration_name, 'bluemix_cos')\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.getOrCreate()\n",
    "df = spark.read\\\n",
    "  .format('org.apache.spark.sql.execution.datasources.csv.CSVFileFormat')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema','true')\\\n",
    "  .load(cos.url('First_Time_Buyer.csv', 'advanceddatasciencecapstone-donotdelete-pr-****'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Cleansing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Missing values treatment / Imputing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSM\n",
    "LSM is Living Standard Measure and it depends on several factors such as income. The missing values can be imputed using the LSM values with the same income_code and/or province or marital status.\n",
    "\n",
    "##  province\n",
    "Province is a categorical variable, as seen previously. The missing province values is considered to be **UNKNOWN**. \n",
    "\n",
    "## years_occupation\n",
    "This column has numerical values. Null values means the idividual does not have any job, so **years_occupation** is set to **ZERO**.\n",
    "\n",
    "## marital_status\n",
    "It's a categorical variable. The missing marital status is set to **UNKNOWN**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------------+-----------+-------+\n",
      "|    province|income_code|med_lsm|\n",
      "+------------+-----------+-------+\n",
      "|Eastern Cape|          1|      3|\n",
      "|Eastern Cape|          2|      3|\n",
      "|Eastern Cape|          3|      5|\n",
      "|Eastern Cape|          4|      6|\n",
      "|Eastern Cape|          5|      7|\n",
      "|Eastern Cape|          6|      8|\n",
      "|Eastern Cape|          7|      9|\n",
      "|Eastern Cape|          8|      9|\n",
      "|Eastern Cape|          9|     10|\n",
      "|Eastern Cape|         10|     10|\n",
      "|Eastern Cape|         11|     10|\n",
      "|  Free State|          1|      3|\n",
      "|  Free State|          2|      3|\n",
      "|  Free State|          3|      5|\n",
      "|  Free State|          4|      6|\n",
      "|  Free State|          5|      7|\n",
      "|  Free State|          6|      8|\n",
      "|  Free State|          7|      9|\n",
      "|  Free State|          8|      9|\n",
      "|  Free State|          9|      9|\n",
      "+------------+-----------+-------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Treating the missing values\n",
    "\n",
    "# province\n",
    "df = df.na.fill(\"Unknown\", 'province')\n",
    "\n",
    "# years_occupation\n",
    "df = df.na.fill(0, 'years_occupation')\n",
    "\n",
    "# marital status\n",
    "df = df.na.fill(\"Unknown\", 'marital_status')\n",
    "\n",
    "\n",
    "# LSM\n",
    "\n",
    "# group by 'province' and 'income_code' and find the median of lsm for each group\n",
    "df.createOrReplaceTempView(\"dftable\")\n",
    "\n",
    "df_lsm = spark.sql(\"\"\"\n",
    "SELECT province, income_code, percentile_approx(lsm, 0.5) AS med_lsm \n",
    "FROM dftable \n",
    "GROUP BY province,income_code \n",
    "ORDER BY province,income_code\n",
    "\"\"\")\n",
    "df_lsm.show()\n",
    "\n",
    "\n",
    "# impute missing values with the calculated median\n",
    "df_lsm.createOrReplaceTempView(\"df_lsm\")\n",
    "\n",
    "df = spark.sql(\"\"\"\n",
    "SELECT d.*, CASE WHEN d.lsm is null THEN l.med_lsm ELSE d.lsm END AS lsm_fixed \n",
    "FROM dftable d\n",
    "LEFT JOIN df_lsm l ON l.province=d.province AND l.income_code=d.income_code\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# drop old lsm column\n",
    "df=df.drop('lsm')\n",
    "\n",
    "# rename lsm_fixed back to lsm\n",
    "df=df.withColumnRenamed('lsm_fixed','lsm')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Remove unnecessary values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown before, all the variables look fine except directorship. I am only interested in an individual is an **active director** or a **resigned director** or **not a director**. So all the other values except **DIRECTORSHIP ACTIVE** and **DIRECTORSHIP RESIGNED** must be changed to **NO DIRECTORSHIP**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new temporary view of DataFrame\n",
    "df.createOrReplaceTempView(\"dftable\")\n",
    "\n",
    "# keep DIRECTORSHIP ACTIVE and DIRECTORSHIP RESIGNED only and change others to NO DIRECTORSHIP\n",
    "df=spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "CASE WHEN (directorship<>'DIRECTORSHIP ACTIVE' AND directorship<>'DIRECTORSHIP RESIGNED') \n",
    "     THEN 'NO DIRECTORSHIP'\n",
    "     ELSE directorship \n",
    "END AS directorship_fixed\n",
    "FROM dftable\n",
    "\"\"\")\n",
    "\n",
    "# drop the old directorship column\n",
    "df=df.drop('directorship')\n",
    "\n",
    "# rename directorship_fixed column to directorship\n",
    "df=df.withColumnRenamed('directorship_fixed','directorship')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Remove correlated variables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown in the **Initial data exploration** notebook, **months_risk_score** is perfectly correlated with **months_income**. So the first one is removed from the DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=df.drop('months_risk_score')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Remove outliers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As shown before, following columns contain outliers.\n",
    "\n",
    "months_mobile_phone\n",
    "\n",
    "months_home_phone\n",
    "\n",
    "months_work_phone\n",
    "\n",
    "years_occupation\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# reset and update outliers\n",
    "\n",
    "df.createOrReplaceTempView('dftable')\n",
    "\n",
    "df=spark.sql(\"\"\"\n",
    "SELECT *,\n",
    "CASE WHEN months_mobile_phone>300 THEN 300 ELSE months_mobile_phone END AS months_mobile_phone_fixed,\n",
    "CASE WHEN months_home_phone>300 THEN 300 ELSE months_home_phone END AS months_home_phone_fixed,\n",
    "CASE WHEN months_work_phone>300 THEN 300 ELSE months_work_phone END AS months_work_phone_fixed,\n",
    "CASE WHEN years_occupation>35 THEN 35 ELSE years_occupation END AS years_occupation_fixed\n",
    "FROM dftable\n",
    "\"\"\")\n",
    "\n",
    "#drop old columns\n",
    "dropped_cols = ['months_mobile_phone','months_home_phone','months_work_phone','years_occupation']\n",
    "df=df.drop(*dropped_cols)\n",
    "\n",
    "# rename the new fixed columns back to the original names\n",
    "for col in dropped_cols:\n",
    "    df=df.withColumnRenamed(col+'_fixed',col)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The project token is an authorization token that is used to access project resources like data sources, connections, and used by platform APIs.\n",
    "\n",
    "from project_lib import Project\n",
    "project = Project(spark.sparkContext, '****', '****')\n",
    "pc = project.project_context"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'file_name': 'First_Time_Buyer_Cleaned.csv',\n",
       " 'message': 'File saved to project storage.',\n",
       " 'bucket_name': 'advanceddatasciencecapstone-donotdelete-pr-nr1zlb0dvlh3em',\n",
       " 'asset_id': '684b0a3d-9af7-4caf-aa52-90ba3fa6408c'}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# save the clean data into a CSV file\n",
    "\n",
    "project.save_data(file_name = \"First_Time_Buyer_Cleaned.csv\",data = df.toPandas().to_csv(index=False),overwrite=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the clean data into Parquet format\n",
    "\n",
    "df.write.parquet(path=cos.url('First_Time_Buyer_Cleaned.parquet', 'advanceddatasciencecapstone-donotdelete-pr-****'), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save the LSM_Imputer table\n",
    "df_lsm.write.parquet(path=cos.url('LSM_Imputer.parquet', 'advanceddatasciencecapstone-donotdelete-pr-****'), mode='overwrite')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6 with Spark",
   "language": "python3",
   "name": "python36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
